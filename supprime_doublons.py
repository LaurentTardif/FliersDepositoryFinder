#!/usr/bin/env python3
"""
Script de suppression des doublons dans les fichiers CSV d'entreprises
Supprime les lignes dupliqu√©es bas√©es sur les colonnes: Nom, Adresse, Ville, Metier
"""

import argparse
import csv
import hashlib
import sys
from typing import Dict, List, Set, Tuple


def normalize_text(text: str) -> str:
    """
    Normalise le texte pour la comparaison de doublons

    Args:
        text: Texte √† normaliser

    Returns:
        Texte normalis√© (minuscules, espaces supprim√©s)
    """
    return text.strip().lower().replace("  ", " ")


def create_record_hash(record: Dict[str, str]) -> str:
    """
    Cr√©e un hash unique pour un enregistrement bas√© sur les champs cl√©s

    Args:
        record: Dictionnaire contenant les donn√©es d'une ligne

    Returns:
        Hash MD5 de l'enregistrement normalis√©
    """
    # Normalisation des champs cl√©s
    nom = normalize_text(record.get("Nom", ""))
    adresse = normalize_text(record.get("Adresse", ""))
    ville = normalize_text(record.get("Ville", ""))
    metier = normalize_text(record.get("Metier_normalise", record.get("Metier", "")))

    # Cr√©ation d'une cl√© composite
    composite_key = f"{nom}|{adresse}|{ville}|{metier}"

    # G√©n√©ration du hash
    return hashlib.md5(composite_key.encode("utf-8")).hexdigest()


def are_similar_records(record1: Dict[str, str], record2: Dict[str, str], similarity_threshold: float = 0.9) -> bool:
    """
    V√©rifie si deux enregistrements sont similaires (optionnel pour d√©tection avanc√©e)

    Args:
        record1: Premier enregistrement
        record2: Deuxi√®me enregistrement
        similarity_threshold: Seuil de similarit√© (non utilis√© dans cette version simple)

    Returns:
        True si les enregistrements sont consid√©r√©s comme des doublons
    """
    # Pour cette version, on utilise une correspondance exacte apr√®s normalisation
    return create_record_hash(record1) == create_record_hash(record2)


def remove_duplicates(input_file: str, output_file: str, verbose: bool = False, sort_by: str = None) -> Tuple[int, int]:
    """
    Supprime les doublons d'un fichier CSV

    Args:
        input_file: Chemin du fichier CSV d'entr√©e
        output_file: Chemin du fichier CSV de sortie
        verbose: Affichage d√©taill√© des op√©rations
        sort_by: Colonne sur laquelle trier (optionnel)

    Returns:
        Tuple (nombre_total, nombre_uniques)
    """
    seen_hashes: Set[str] = set()
    unique_records: List[Dict[str, str]] = []
    total_records = 0
    duplicate_records = 0

    try:
        # Lecture du fichier d'entr√©e
        with open(input_file, "r", encoding="utf-8") as file:
            reader = csv.DictReader(file)

            # D√©tection automatique des colonnes d'entr√©e
            input_fieldnames = reader.fieldnames
            if not input_fieldnames:
                raise ValueError("Impossible de lire les colonnes du fichier d'entr√©e")

            print(f"Colonnes d√©tect√©es dans le fichier d'entr√©e: {input_fieldnames}")

            # V√©rification des colonnes requises pour la d√©tection de doublons
            required_base_columns = {"Nom", "Adresse", "Ville"}
            if not required_base_columns.issubset(set(input_fieldnames)):
                missing = required_base_columns - set(input_fieldnames)
                print(f"Colonnes disponibles: {input_fieldnames}")
                print(f"Colonnes minimales requises: {required_base_columns}")
                raise ValueError(f"Colonnes manquantes: {missing}")

            # V√©rifier qu'on a au moins une colonne m√©tier
            has_metier = "Metier" in input_fieldnames
            has_metier_normalise = "Metier_normalise" in input_fieldnames
            if not (has_metier or has_metier_normalise):
                raise ValueError("Aucune colonne 'Metier' ou 'Metier_normalise' trouv√©e")

            for row_num, record in enumerate(reader, 1):
                total_records += 1

                # Cr√©ation du hash pour ce record
                record_hash = create_record_hash(record)

                if record_hash not in seen_hashes:
                    # Nouvel enregistrement unique
                    seen_hashes.add(record_hash)
                    unique_records.append(record)

                    if verbose:
                        print(f"[{row_num}] Unique: {record.get('Nom', '')[:30]}...")
                else:
                    # Doublon d√©tect√©
                    duplicate_records += 1

                    if verbose:
                        print(f"[{row_num}] Doublon supprim√©: {record.get('Nom', '')[:30]}...")

        # Tri des donn√©es si demand√©
        if sort_by:
            if unique_records and sort_by in unique_records[0]:
                if verbose:
                    print(f"üìä Tri des donn√©es par '{sort_by}'...")
                unique_records = sorted(unique_records, key=lambda x: x.get(sort_by, "").lower())
                if verbose:
                    print(f"   Premier: {unique_records[0].get(sort_by, '')}")
                    print(f"   Dernier: {unique_records[-1].get(sort_by, '')}")
            else:
                available_columns = list(unique_records[0].keys()) if unique_records else []
                print(f"‚ö†Ô∏è  Colonne '{sort_by}' non trouv√©e. Colonnes disponibles: {', '.join(available_columns)}")
                print("   Tri ignor√©, suppression des doublons effectu√©e.")

        # √âcriture du fichier de sortie avec toutes les colonnes d√©tect√©es
        output_fieldnames = input_fieldnames  # Conserver toutes les colonnes d'entr√©e

        with open(output_file, "w", newline="", encoding="utf-8") as file:
            writer = csv.DictWriter(file, fieldnames=output_fieldnames)
            writer.writeheader()

            for record in unique_records:
                # Cr√©er un enregistrement avec toutes les colonnes pr√©sentes
                clean_record = {col: record.get(col, "") for col in output_fieldnames}
                writer.writerow(clean_record)

        return total_records, len(unique_records)

    except FileNotFoundError:
        print(f"Erreur: Fichier d'entr√©e '{input_file}' non trouv√©")
        sys.exit(1)
    except Exception as e:
        print(f"Erreur lors du traitement: {e}")
        sys.exit(1)


def analyze_duplicates(input_file: str):
    """
    Analyse les doublons sans les supprimer (mode analyse)

    Args:
        input_file: Chemin du fichier CSV √† analyser
    """
    hash_count: Dict[str, int] = {}
    hash_examples: Dict[str, Dict[str, str]] = {}
    total_records = 0

    try:
        with open(input_file, "r", encoding="utf-8") as file:
            reader = csv.DictReader(file)

            for record in reader:
                total_records += 1
                record_hash = create_record_hash(record)

                if record_hash in hash_count:
                    hash_count[record_hash] += 1
                else:
                    hash_count[record_hash] = 1
                    hash_examples[record_hash] = record

        # Affichage des statistiques
        duplicates = {h: count for h, count in hash_count.items() if count > 1}
        unique_count = len([h for h, count in hash_count.items() if count == 1])

        print(f"\nüìä Analyse des doublons:")
        print(f"   Total d'enregistrements: {total_records}")
        print(f"   Enregistrements uniques: {unique_count}")
        print(f"   Groupes de doublons: {len(duplicates)}")
        print(f"   Total de doublons: {sum(duplicates.values()) - len(duplicates)}")

        if duplicates:
            print(f"\nüîç Exemples de doublons:")
            for i, (hash_key, count) in enumerate(list(duplicates.items())[:5], 1):
                example = hash_examples[hash_key]
                print(f"   {i}. {example.get('Nom', '')[:40]} ({count} occurrences)")

    except Exception as e:
        print(f"Erreur lors de l'analyse: {e}")
        sys.exit(1)


def main():
    parser = argparse.ArgumentParser(
        description="Suppression des doublons dans les fichiers CSV d'entreprises",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Exemples d'utilisation:
  python supprime_doublons.py input.csv output.csv
  python supprime_doublons.py input.csv output.csv --verbose --sort Nom
  python supprime_doublons.py input.csv output.csv --sort Note
  python supprime_doublons.py input.csv --analyze

Le script compare les enregistrements sur la base des colonnes:
  - Nom (normalis√©)
  - Adresse (normalis√©e)
  - Ville (normalis√©e)
  - Metier ou Metier_normalise (normalis√©)

Ce script conserve automatiquement toutes les colonnes pr√©sentes dans le fichier d'entr√©e.
Cela inclut les nouvelles colonnes comme Heures_ouverture, Nombre_avis, Note, Jours_fermeture.

Options de tri disponibles: toutes les colonnes d√©tect√©es dans le fichier
La normalisation supprime les espaces en trop et convertit en minuscules.
        """,
    )

    parser.add_argument("input_file", help="Fichier CSV d'entr√©e")
    parser.add_argument("output_file", nargs="?", help="Fichier CSV de sortie (requis sauf avec --analyze)")
    parser.add_argument("--verbose", "-v", action="store_true", help="Affichage d√©taill√© des op√©rations")
    parser.add_argument(
        "--analyze", "-a", action="store_true", help="Mode analyse: affiche les statistiques sans cr√©er de fichier de sortie"
    )
    parser.add_argument("--sort", "-s", metavar="COLUMN", help="Trier les r√©sultats par colonne (ex: Nom, Ville, Metier)")

    args = parser.parse_args()

    # Validation des arguments
    if not args.analyze and not args.output_file:
        print("Erreur: fichier de sortie requis (sauf avec --analyze)")
        parser.print_help()
        sys.exit(1)

    if args.analyze:
        # Mode analyse
        print(f"üîç Analyse des doublons dans: {args.input_file}")
        analyze_duplicates(args.input_file)
    else:
        # Mode suppression
        print(f"üßπ Suppression des doublons...")
        print(f"   Entr√©e: {args.input_file}")
        print(f"   Sortie: {args.output_file}")
        if args.sort:
            print(f"   Tri par: {args.sort}")

        total, unique = remove_duplicates(args.input_file, args.output_file, args.verbose, args.sort)
        duplicates_removed = total - unique

        print(f"\n‚úÖ Traitement termin√©:")
        print(f"   Enregistrements trait√©s: {total}")
        print(f"   Enregistrements uniques: {unique}")
        print(f"   Doublons supprim√©s: {duplicates_removed}")

        if total > 0:
            reduction_percent = (duplicates_removed / total) * 100
            print(f"   R√©duction: {reduction_percent:.1f}%")


if __name__ == "__main__":
    main()

# Exemples de fichiers CSV compatibles:
#
# Format √©tendu (avec nouvelles colonnes) - input.csv:
# Nom,Adresse,Ville,Metier,Heures_ouverture,Nombre_avis,Note,Jours_fermeture
# Jean Dupont,12 rue A,Paris,boulanger,"lundi: 07:00-19:00",142,4.3,1
# Marie Martin,34 rue B,Lyon,plombier,"lundi: 08:00-18:00",89,4.1,1
# Jean Dupont,12 rue A,Paris,boulanger,"lundi: 07:00-19:00",142,4.3,1
# Sophie Bernard,78 rue D,Toulouse,enseignant,Non disponible,45,4.2,2
#
# R√©sultat apr√®s suppression des doublons (toutes colonnes conserv√©es):
# Nom,Adresse,Ville,Metier,Heures_ouverture,Nombre_avis,Note,Jours_fermeture
# Jean Dupont,12 rue A,Paris,boulanger,"lundi: 07:00-19:00",142,4.3,1
# Marie Martin,34 rue B,Lyon,plombier,"lundi: 08:00-18:00",89,4.1,1
# Sophie Bernard,78 rue D,Toulouse,enseignant,Non disponible,45,4.2,2
#
# Format classique (ancien format) - input.csv:
# Nom,Adresse,Ville,Metier
# Jean Dupont,12 rue A,Paris,boulanger
# Marie Martin,34 rue B,Lyon,plombier
# Jean Dupont,12 rue A,Paris,boulanger
# Sophie Bernard,78 rue D,Toulouse,enseignant
#
# R√©sultat apr√®s suppression des doublons:
# Nom,Adresse,Ville,Metier
# Jean Dupont,12 rue A,Paris,boulanger
# Marie Martin,34 rue B,Lyon,plombier
# Sophie Bernard,78 rue D,Toulouse,enseignant
